{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "# diffusionlab imports\n",
        "from diffusionlab.dynamics import VariancePreservingProcess\n",
        "from diffusionlab.losses import DiffusionLoss\n",
        "from diffusionlab.vector_fields import VectorFieldType\n",
        "\n",
        "# repo imports\n",
        "from src.diffusion_mem_gen.models.gmm import (\n",
        "    IsoHomGMMInitStrategy,\n",
        "    iso_hom_gmm_create_initialization_parameters,\n",
        "    IsoHomGMMSharedParametersEstimator,\n",
        ")\n",
        "from src.diffusion_mem_gen.utils.factories import compute_loss_factory\n",
        "from investigating_diffusion_loss import AmbientDiffusionLoss\n",
        "\n",
        "key = random.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crossover Analysis: How crossover point varies with N\n",
        "\n",
        "This notebook investigates how the crossover point (where PMEM loss equals Generalising loss) varies with different values of N (number of training samples).\n",
        "\n",
        "We create a 3D plot with:\n",
        "- x-axis: t_n values\n",
        "- y-axis: N values (50, 100, 150, 300, 600, 1000)\n",
        "- z-axis: M/N at crossover point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dimension and other fixed parameters\n",
        "laboratory_d = 100  # data dimension\n",
        "laboratory_K = 12  # number of components in the GMM\n",
        "\n",
        "u_means_variance = 30 ** (1/2)  # order 1, controls the variance of the means of the true distribution\n",
        "sample_variance = 1  # isotropic variance for each component\n",
        "\n",
        "# Diffusion process (needed for L_N function)\n",
        "diffusion_process = VariancePreservingProcess()\n",
        "\n",
        "# N values to investigate\n",
        "N_values = [50, 100, 150, 300, 600, 1000]\n",
        "\n",
        "# M values: range to search for crossover (will be adjusted per N)\n",
        "M_points = 10  # number of M values to evaluate\n",
        "\n",
        "# t_n values to vary (reduced by half: from 17 to ~8-9)\n",
        "t_n_values = np.linspace(0.01, 0.99, 9)  # Reduced from 17 to 9\n",
        "print(f\"t_n values: {t_n_values}\")\n",
        "\n",
        "# Time grid for evaluation\n",
        "t_val_array = jnp.linspace(0.01, 0.99, 26)\n",
        "\n",
        "# Lambda function (constant for now)\n",
        "lambda_fn = lambda t: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def L_N(model_callable, lambda_fn, t_val_array, t_n_value, X_0_dataset, X_t_n_dataset, key):\n",
        "    '''\n",
        "    This function computes the L_N loss for a given model and a given lambda function.\n",
        "    '''\n",
        "    ambient_loss_obj = AmbientDiffusionLoss(diffusion_process, num_noise_draws_per_sample=1, t_n=t_n_value)\n",
        "    standard_loss_obj = DiffusionLoss(diffusion_process, vector_field_type=VectorFieldType.X0, num_noise_draws_per_sample=1)\n",
        "\n",
        "    def L_N_t_ambient(model_callable, t_n_value, t_val, X_t_n_dataset, key):\n",
        "        '''\n",
        "        This function computes the L_N_t for the ambient denoising term.\n",
        "        '''\n",
        "        assert t_val > t_n_value\n",
        "        \n",
        "        compute_loss = compute_loss_factory(ambient_loss_obj, jnp.array(t_val))\n",
        "        loss_val = compute_loss(key, model_callable, X_t_n_dataset) / X_0_dataset.shape[0] \n",
        "        \n",
        "        return loss_val\n",
        "\n",
        "    def L_N_t_standard_score(model_callable, t_val, X_t_dataset, key):\n",
        "        '''\n",
        "        This function computes the L_N_t for the standard denoising term.\n",
        "        '''\n",
        "        compute_loss = compute_loss_factory(standard_loss_obj, jnp.array(t_val))\n",
        "        loss_val = compute_loss(key, model_callable, X_t_dataset) / X_0_dataset.shape[0] \n",
        "        \n",
        "        return loss_val\n",
        "    \n",
        "    # Split into less than t_n and greater than t_n\n",
        "    less_than_n_mask = jnp.less_equal(t_val_array, t_n_value)\n",
        "    greater_than_n_mask = jnp.logical_not(less_than_n_mask)\n",
        "\n",
        "    standard_denoising_t_values = t_val_array[less_than_n_mask]\n",
        "    ambient_denoising_t_values = t_val_array[greater_than_n_mask]\n",
        "  \n",
        "    # Compute the loss for the standard denoising term\n",
        "    current_key = key\n",
        "    standard_denoising_loss_values = []\n",
        "    for t in standard_denoising_t_values:\n",
        "        current_key, subk = random.split(current_key)\n",
        "        loss_val = L_N_t_standard_score(model_callable, t, X_0_dataset, subk)\n",
        "        standard_denoising_loss_values.append(lambda_fn(t) * loss_val)\n",
        "\n",
        "    averaged_standard_denoising_loss = t_n_value * np.mean(standard_denoising_loss_values)\n",
        "\n",
        "    # Compute the loss for the ambient denoising term\n",
        "    ambient_denoising_loss_values = []\n",
        "    for t in ambient_denoising_t_values:\n",
        "        current_key, subk = random.split(current_key)\n",
        "        loss_val = L_N_t_ambient(model_callable, t_n_value, t, X_t_n_dataset, subk)\n",
        "        ambient_denoising_loss_values.append(lambda_fn(t) * loss_val)\n",
        "    \n",
        "    averaged_ambient_denoising_loss = (1 - t_n_value) * np.mean(ambient_denoising_loss_values)\n",
        "\n",
        "    return averaged_standard_denoising_loss + averaged_ambient_denoising_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_crossover_point(M_values, pmem_losses, generalising_losses, N):\n",
        "    '''\n",
        "    Find the M value where PMEM and Generalising losses cross over.\n",
        "    Returns the M/N ratio at crossover, or None if no crossover found.\n",
        "    '''\n",
        "    loss_difference = pmem_losses - generalising_losses\n",
        "    \n",
        "    # Check if there's a sign change\n",
        "    sign_changes = np.where(np.diff(np.sign(loss_difference)))[0]\n",
        "    \n",
        "    if len(sign_changes) == 0:\n",
        "        # No sign change - check if we're close to zero\n",
        "        min_abs_diff_idx = np.argmin(np.abs(loss_difference))\n",
        "        min_abs_diff = np.abs(loss_difference[min_abs_diff_idx])\n",
        "        \n",
        "        # If we're very close to zero, return that point\n",
        "        if min_abs_diff < 0.01:  # threshold for \"close enough\"\n",
        "            return M_values[min_abs_diff_idx] / N\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    # Interpolate to find exact crossover point\n",
        "    crossover_M_over_N = []\n",
        "    for idx in sign_changes:\n",
        "        # Linear interpolation between points\n",
        "        x1, x2 = M_values[idx] / N, M_values[idx + 1] / N\n",
        "        y1, y2 = loss_difference[idx], loss_difference[idx + 1]\n",
        "        \n",
        "        # Find where y = 0\n",
        "        if y2 != y1:\n",
        "            x_cross = x1 - y1 * (x2 - x1) / (y2 - y1)\n",
        "            crossover_M_over_N.append(x_cross)\n",
        "    \n",
        "    # Return the first crossover point (or average if multiple)\n",
        "    if len(crossover_M_over_N) > 0:\n",
        "        return crossover_M_over_N[0]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for all (N, t_n) combinations\n",
        "all_crossover_data = []  # List of (N, t_n, M/N at crossover)\n",
        "\n",
        "# Evaluate for each N value\n",
        "for laboratory_N in N_values:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing N = {laboratory_N}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # M values: range to search for crossover (adjusted per N)\n",
        "    M_values = np.linspace(10, laboratory_N, M_points).astype(int)\n",
        "    print(f\"M values: {M_values}\")\n",
        "    \n",
        "    # Ground-truth GMM params\n",
        "    key, sk = random.split(key)\n",
        "    true_means = random.normal(sk, (laboratory_K, laboratory_d)) * u_means_variance\n",
        "    equal_weighted_prior = jnp.array([1/laboratory_K for _ in range(laboratory_K)])  # must sum to 1\n",
        "\n",
        "    # Sample training set from the true GMM\n",
        "    key, sk = random.split(key)\n",
        "    comp_ids = random.choice(sk, laboratory_K, shape=(laboratory_N,), p=equal_weighted_prior)\n",
        "    key, sk = random.split(key)\n",
        "    X_train = true_means[comp_ids] + jnp.sqrt(sample_variance) * random.normal(sk, (laboratory_N, laboratory_d))\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    \n",
        "    # Build Generalizing denoiser (same for all M and t_n values)\n",
        "    generalising_model = IsoHomGMMSharedParametersEstimator(\n",
        "        dim=laboratory_d,\n",
        "        num_components=laboratory_K,\n",
        "        vf_type=VectorFieldType.X0,\n",
        "        diffusion_process=diffusion_process,\n",
        "        init_means=true_means,\n",
        "        init_var=jnp.asarray(sample_variance),\n",
        "        priors=equal_weighted_prior,\n",
        "    )\n",
        "    \n",
        "    # Evaluate for each t_n value\n",
        "    for t_n in t_n_values:\n",
        "        print(f\"\\n  Processing t_n = {t_n:.3f}...\", end=\" \")\n",
        "        \n",
        "        t_n_value = jnp.array(t_n)\n",
        "        \n",
        "        # Generate X_t_n_dataset for this t_n value\n",
        "        key, subk = random.split(key)\n",
        "        X_t_n_eps = jax.random.normal(subk, X_train.shape)\n",
        "        batch_diffusion_forward = jax.vmap(\n",
        "            diffusion_process.forward, in_axes=(0, None, 0)\n",
        "        )\n",
        "        X_t_n_dataset = batch_diffusion_forward(X_train, t_n_value, X_t_n_eps)\n",
        "        \n",
        "        # Store losses for this t_n\n",
        "        pmem_losses = []\n",
        "        generalising_losses = []\n",
        "        \n",
        "        # Evaluate L_N for each M value\n",
        "        for M in M_values:\n",
        "            # Build PMEM denoiser for this M value\n",
        "            key, sk = random.split(key)\n",
        "            context = {\n",
        "                \"X_train\": X_train,\n",
        "                \"init_var_scale\": 1e-6,\n",
        "                \"init_means_noise_var\": 0.0,\n",
        "            }\n",
        "            means_pmem, var_pmem, priors_pmem = iso_hom_gmm_create_initialization_parameters(\n",
        "                sk, IsoHomGMMInitStrategy.PMEM, laboratory_d, M, context\n",
        "            )\n",
        "            \n",
        "            pmem_model = IsoHomGMMSharedParametersEstimator(\n",
        "                dim=laboratory_d,\n",
        "                num_components=M,\n",
        "                vf_type=VectorFieldType.X0,\n",
        "                diffusion_process=diffusion_process,\n",
        "                init_means=means_pmem,\n",
        "                init_var=var_pmem,\n",
        "                priors=priors_pmem,\n",
        "            )\n",
        "            \n",
        "            # Evaluate L_N for PMEM model\n",
        "            key, subk = random.split(key)\n",
        "            pmem_loss = L_N(pmem_model, lambda_fn, t_val_array, t_n_value, X_train, X_t_n_dataset, subk)\n",
        "            pmem_losses.append(float(pmem_loss))\n",
        "            \n",
        "            # Evaluate L_N for Generalizing model (same for all M)\n",
        "            key, subk = random.split(key)\n",
        "            generalising_loss = L_N(generalising_model, lambda_fn, t_val_array, t_n_value, X_train, X_t_n_dataset, subk)\n",
        "            generalising_losses.append(float(generalising_loss))\n",
        "        \n",
        "        pmem_losses = np.array(pmem_losses)\n",
        "        generalising_losses = np.array(generalising_losses)\n",
        "        \n",
        "        # Find crossover point\n",
        "        crossover_M_over_N = find_crossover_point(M_values, pmem_losses, generalising_losses, laboratory_N)\n",
        "        if crossover_M_over_N is not None and not np.isnan(crossover_M_over_N):\n",
        "            all_crossover_data.append((laboratory_N, t_n, crossover_M_over_N))\n",
        "            print(f\"Crossover at M/N = {crossover_M_over_N:.4f}\")\n",
        "        else:\n",
        "            print(\"No crossover found\")\n",
        "    \n",
        "    print(f\"\\nCompleted N = {laboratory_N}\")\n",
        "\n",
        "print(f\"\\n\\nTotal crossover points found: {len(all_crossover_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for 3D plotting\n",
        "if len(all_crossover_data) > 0:\n",
        "    # Extract data\n",
        "    N_array = np.array([d[0] for d in all_crossover_data])\n",
        "    t_n_array = np.array([d[1] for d in all_crossover_data])\n",
        "    M_over_N_array = np.array([d[2] for d in all_crossover_data])\n",
        "    \n",
        "    # Create 3D plot\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    \n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(t_n_array, N_array, M_over_N_array, \n",
        "                        c=M_over_N_array, cmap='viridis', \n",
        "                        s=50, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter, ax=ax, pad=0.1)\n",
        "    cbar.set_label('M/N at Crossover', fontsize=11)\n",
        "    \n",
        "    # Set labels\n",
        "    ax.set_xlabel('$t_n$', fontsize=12, labelpad=10)\n",
        "    ax.set_ylabel('$N$ (Number of Training Samples)', fontsize=12, labelpad=10)\n",
        "    ax.set_zlabel('Crossover $M/N$', fontsize=12, labelpad=10)\n",
        "    ax.set_title('Crossover Point Variation with $N$ and $t_n$', fontsize=14, pad=20)\n",
        "    \n",
        "    # Set y-axis ticks to match N_values\n",
        "    ax.set_yticks(N_values)\n",
        "    \n",
        "    # Improve viewing angle\n",
        "    ax.view_init(elev=20, azim=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\nCrossover points summary:\")\n",
        "    print(f\"{'N':<6} {'t_n':<8} {'M/N':<10}\")\n",
        "    print(\"-\" * 25)\n",
        "    for N, t_n, m_over_n in all_crossover_data:\n",
        "        print(f\"{N:<6} {t_n:<8.3f} {m_over_n:<10.4f}\")\n",
        "else:\n",
        "    print(\"No crossover points found. The losses may not cross in the evaluated range.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Surface plot if we have enough data points\n",
        "if len(all_crossover_data) > 0:\n",
        "    # Organize data into a grid for surface plotting\n",
        "    # Create meshgrid for N and t_n\n",
        "    N_unique = np.array(N_values)\n",
        "    t_n_unique = t_n_values\n",
        "    \n",
        "    # Create a grid to store M/N values\n",
        "    M_over_N_grid = np.full((len(N_unique), len(t_n_unique)), np.nan)\n",
        "    \n",
        "    # Fill in the grid with crossover points\n",
        "    for N, t_n, m_over_n in all_crossover_data:\n",
        "        N_idx = np.where(N_unique == N)[0][0]\n",
        "        t_n_idx = np.where(np.isclose(t_n_unique, t_n))[0][0]\n",
        "        M_over_N_grid[N_idx, t_n_idx] = m_over_n\n",
        "    \n",
        "    # Create meshgrid\n",
        "    T_n_grid, N_grid = np.meshgrid(t_n_unique, N_unique)\n",
        "    \n",
        "    # Create surface plot\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    \n",
        "    # Plot surface (only where we have data)\n",
        "    surf = ax.plot_surface(T_n_grid, N_grid, M_over_N_grid, \n",
        "                          cmap='viridis', alpha=0.8, \n",
        "                          linewidth=0, antialiased=True,\n",
        "                          edgecolor='none')\n",
        "    \n",
        "    # Add scatter points on top\n",
        "    N_array = np.array([d[0] for d in all_crossover_data])\n",
        "    t_n_array = np.array([d[1] for d in all_crossover_data])\n",
        "    M_over_N_array = np.array([d[2] for d in all_crossover_data])\n",
        "    ax.scatter(t_n_array, N_array, M_over_N_array, \n",
        "              c='red', s=30, alpha=1.0, edgecolors='black', linewidth=0.5)\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(surf, ax=ax, pad=0.1)\n",
        "    cbar.set_label('M/N at Crossover', fontsize=11)\n",
        "    \n",
        "    # Set labels\n",
        "    ax.set_xlabel('$t_n$', fontsize=12, labelpad=10)\n",
        "    ax.set_ylabel('$N$ (Number of Training Samples)', fontsize=12, labelpad=10)\n",
        "    ax.set_zlabel('Crossover $M/N$', fontsize=12, labelpad=10)\n",
        "    ax.set_title('Crossover Point Surface: Variation with $N$ and $t_n$', fontsize=14, pad=20)\n",
        "    \n",
        "    # Set y-axis ticks to match N_values\n",
        "    ax.set_yticks(N_values)\n",
        "    \n",
        "    # Improve viewing angle\n",
        "    ax.view_init(elev=25, azim=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
